<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning Real-world Visuo-motor Policies from Simulation | Fangyi Zhang </title> <meta name="author" content="Fangyi Zhang"> <meta name="description" content="This is my Ph.D. project in the Australian Centre for Robotic Vision at QUT, with supervisions from Prof. Peter Corke, Dr. Jürgen Leitner, Prof. Michael Milford and Dr. Ben Upcroft."> <meta name="keywords" content="robotics, AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/robot_x.png?350742e6cec5788ce59ac83f990524b0"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.fangyizhang.com/projects/1_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Fangyi</span> Zhang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <article> <h3><strong>Learning Real-world Visuo-motor Policies from Simulation</strong></h3> <p>May 31, 2018</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_1-480.webp 480w,/assets/img/project_1-800.webp 800w,/assets/img/project_1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/project_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>This is my Ph.D. project in the <a href="https://www.roboticvision.org/" target="_blank" rel="noopener">Australian Centre for Robotic Vision</a> at <a href="https://wiki.qut.edu.au/display/cyphy/Robotics@QUT" target="_blank" rel="noopener">QUT</a>, with supervisions from <a href="https://wiki.qut.edu.au/display/cyphy/Peter+Corke" target="_blank" rel="noopener">Prof. Peter Corke</a>, <a href="http://juxi.net/" target="_blank" rel="noopener">Dr. Jürgen Leitner</a>, <a href="https://wiki.qut.edu.au/display/cyphy/Michael+Milford" target="_blank" rel="noopener">Prof. Michael Milford</a> and <a href="https://www.roboticvision.org/rv_person/ben-upcroft/" target="_blank" rel="noopener">Dr. Ben Upcroft</a>.</p> <h4 id="learning-planar-reaching-in-simulation"><strong>Learning Planar Reaching in Simulation</strong></h4> <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"> <iframe src="https://www.youtube.com/embed/6cz-mcM4Qkc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen="" title="YouTube Video"></iframe> </div> <h4 class="mt-3" id="robotic-planar-reaching-in-the-real-world"><strong>Robotic Planar Reaching in the Real World</strong></h4> <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"> <iframe src="https://www.youtube.com/embed/ybuFdsE6AjY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen="" title="YouTube Video"></iframe> </div> <h4 class="mt-3" id="learning-table-top-object-reaching-with-a-7-dof-robotic-arm-from-simulation"><strong>Learning Table-top Object Reaching with a 7 DoF Robotic Arm from Simulation</strong></h4> <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"> <iframe src="https://www.youtube.com/embed/bVIw1DeuuYg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen="" title="YouTube Video"></iframe> </div> <p class="mt-3">Contributions:</p> <ul> <li>Feasibility analysis on learning vision-based robotic planar reaching using DQNs in simulation <a class="citation" href="#zhang2015towards">(Zhang et al., 2015)</a>.</li> <li>Proposed a modular deep Q network architecture for fast and low-cost transfer of visuo-motor policies from simulation to the real world <a class="citation" href="#zhang2017acra">(Zhang et al., 2017)</a>.</li> <li>Proposed an end-to-end fine-tuning method using weighted losses to improve hand-eye coordination <a class="citation" href="#zhang2017cvpr">(Zhang et al., 2017)</a>.</li> <li>Proposed a kinematics-based guided policy search method (K-GPS) to speed up Q learning for robotic applications where kinematic models are known <a class="citation" href="#zhang2017acra">(Zhang et al., 2017)</a>.</li> <li>Demonstrated in robotic reaching tasks on a real Baxter robot in velocity and position control modes, e.g., table-top object reaching in clutter <a class="citation" href="#zhang2019adversarial">(Zhang et al., 2019)</a> and planar reaching <a class="citation" href="#zhang2017acra">(Zhang et al., 2017)</a>.</li> <li>More investigations are undergoing for semi-supervised and unsupervised transfer from simulation to the real world using adversarial discriminative approaches <a class="citation" href="#zhang2019adversarial">(Zhang et al., 2019)</a>.</li> </ul> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Adversarial_discriminative-480.webp 480w,/assets/img/publication_preview/Adversarial_discriminative-800.webp 800w,/assets/img/publication_preview/Adversarial_discriminative-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Adversarial_discriminative.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Adversarial_discriminative.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2019adversarial" class="col-sm-8"> <div class="title">Adversarial discriminative sim-to-real transfer of visuo-motor policies</div> <div class="author"> <em>Fangyi Zhang</em>, Jürgen Leitner, Zongyuan Ge, Michael Milford, and Peter Corke </div> <div class="periodical"> <em>The International Journal of Robotics Research (IJRR)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1709.05746" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Fanleyrobot/ADT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Various approaches have been proposed to learn visuo-motor policies for real-world robotic applications. One solution is first learning in simulation then transferring to the real world. In the transfer, most existing approaches need real-world images with labels. However, the labelling process is often expensive or even impractical in many robotic applications. In this paper, we propose an adversarial discriminative sim-to-real transfer approach to reduce the cost of labelling real data. The effectiveness of the approach is demonstrated with modular networks in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The adversarial transfer approach reduced the labelled real data requirement by 50%. Policies can be transferred to real environments with only 93 labelled and 186 unlabelled real images. The transferred visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 97.8% success rate and 1.8 cm control accuracy.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Modular_Deep_Q-480.webp 480w,/assets/img/publication_preview/Modular_Deep_Q-800.webp 800w,/assets/img/publication_preview/Modular_Deep_Q-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Modular_Deep_Q.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Modular_Deep_Q.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2017acra" class="col-sm-8"> <div class="title">Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies</div> <div class="author"> <em>Fangyi Zhang</em>, Jürgen Leitner, Michael Milford, and Peter Corke </div> <div class="periodical"> <em>In Australasian Conference on Robotics and Automation (ACRA)</em>, Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1610.06781" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>While deep learning has had significant successes in computer vision thanks to the abundance of visual data, collecting sufficiently large real-world datasets for robot learning can be costly. To increase the practicality of these techniques on real robots, we propose a modular deep reinforcement learning method capable of transferring models trained in simulation to a real-world robotic task. We introduce a bottleneck between perception and control, enabling the networks to be trained independently, but then merged and fine-tuned in an end-to-end manner to further improve hand-eye coordination. On a canonical, planar visually-guided robot reaching task a fine-tuned accuracy of 1.6 pixels is achieved, a significant improvement over naive transfer (17.5 pixels), showing the potential for more complicated and broader applications. Our method provides a technique for more efficient learning and transfer of visuo-motor policies for real robotic systems without relying entirely on large real-world robot datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Tuning_Modular-480.webp 480w,/assets/img/publication_preview/Tuning_Modular-800.webp 800w,/assets/img/publication_preview/Tuning_Modular-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Tuning_Modular.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Tuning_Modular.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2017cvpr" class="col-sm-8"> <div class="title">Tuning Modular Networks With Weighted Losses for Hand-Eye Coordination</div> <div class="author"> <em>Fangyi Zhang</em>, Jürgen Leitner, Michael Milford, and Peter Corke </div> <div class="periodical"> <em>In IEEE Conference on Computer Vision and Pattern Recognition Workshops</em>, Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1705.05116" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces an end-to-end fine-tuning method to improve hand-eye coordination in modular deep visuo-motor policies (modular networks) where each module is trained independently. Benefiting from weighted losses, the fine-tuning method significantly improves the performance of the policies for a robotic planar reaching task.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Towards_Vision_Based-480.webp 480w,/assets/img/publication_preview/Towards_Vision_Based-800.webp 800w,/assets/img/publication_preview/Towards_Vision_Based-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Towards_Vision_Based.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Towards_Vision_Based.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2015towards" class="col-sm-8"> <div class="title">Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control</div> <div class="author"> <em>Fangyi Zhang</em>, Jürgen Leitner, Michael Milford, Ben Upcroft, and Peter Corke </div> <div class="periodical"> <em>In Australasian Conference on Robotics and Automation (ACRA)</em>, Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1511.03791" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.</p> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Fangyi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1Y8LRS1FDK"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1Y8LRS1FDK");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"Fangyi Zhang&#39;s CV.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/05/01/tabs.html"}},{id:"news-our-paper-re-evaluating-parallel-finger-tip-tactile-sensing-for-inferring-object-adjectives-an-empirical-study-was-presented-at-iros-2023",title:"Our paper \u201cRe-Evaluating Parallel Finger-Tip Tactile Sensing for Inferring Object Adjectives: An Empirical...",description:"",section:"News"},{id:"news-our-paper-towards-assessing-compliant-robotic-grasping-from-first-object-perspective-via-instrumented-objects-got-accepted-to-ra-l",title:"Our paper \u201cTowards assessing compliant robotic grasping from first-object perspective via instrumented objects\u201d...",description:"",section:"News"},{id:"news-our-research-on-learning-fabric-manipulation-in-the-real-world-with-human-videos-was-presented-at-icra-2024",title:"Our research on \u201clearning fabric manipulation in the real world with human videos\u201d...",description:"",section:"News"},{id:"news-our-4th-workshop-on-representing-and-manipulating-deformable-objects-will-be-held-at-icra-2024-on-the-17th-of-may-in-pacifico-yokohama-north-room-g304",title:"Our 4th Workshop on Representing and Manipulating Deformable Objects will be held at...",description:"",section:"News"},{id:"news-our-ra-l-paper-towards-assessing-compliant-robotic-grasping-from-first-object-perspective-via-instrumented-objects-will-be-presented-at-the-40th-anniversary-of-the-ieee-conference-on-robotics-and-automation-icra-40",title:"Our RA-L paper \u201cTowards assessing compliant robotic grasping from first-object perspective via instrumented...",description:"",section:"News"},{id:"news-our-research-on-assessing-robotic-grasping-through-instrumented-objects-has-been-featured-by-qut-news-a-little-help-for-robots-that-don-t-know-their-own-strength",title:"Our research on \u201cassessing robotic grasping through instrumented objects\u201d has been featured by...",description:"",section:"News"},{id:"projects-learning-real-world-visuo-motor-policies-from-simulation",title:"Learning Real-world Visuo-motor Policies from Simulation",description:"This is my Ph.D. project in the Australian Centre for Robotic Vision at QUT, with supervisions from Prof. Peter Corke, Dr. J\xfcrgen Leitner, Prof. Michael Milford and Dr. Ben Upcroft.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-robotic-manipulation-for-warehouse-and-household-applications",title:"Robotic Manipulation for Warehouse and Household Applications",description:"Amazon Picking Challenge (2016) As part of the Team ACRV for the Amazon Picking Challenge 2016, I worked on hand-eye calibration with Dr. Leo Wu. Household Applications This is a project I worked on during my visit to the Perception and Robotics Group at the University of Maryland, College Park, Sep-Dec 2016.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-vlc-based-indoor-localization",title:"VLC-Based Indoor Localization",description:"This is a project I worked on with Prof. Ming Liu and Mr. Kejie Qiu when I was a research assistant in the RAM-LAB at HKUST. The project developed a beacon code selection algorithm and a decomposition algorithm for blindly mixed beacon signals, based on CDMA code selection principles and Gold-sequence correlation properties.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-tactile-based-robotic-manipulation",title:"Tactile-based Robotic Manipulation",description:"A project exploring tactile sensing and tactile-based robotic manipulation.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-assessing-compliant-robotic-manipulation-with-instrumented-objects",title:"Assessing Compliant Robotic Manipulation with Instrumented Objects",description:"A project investigating using instrumented objects to assess compliant robotic manipulation performance.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-deep-learning-and-its-utility-for-data-mining-and-computer-vision",title:"Deep Learning and Its Utility for Data Mining and Computer Vision",description:"This is the research I did at the Alibaba DAMO Academy.",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%64%72.%66%61%6E%67%79%69.%7A%68%61%6E%67@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0003-3938-5377","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=5jFI06UAAAAJ","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Fangyi_Zhang/","_blank")}},{id:"socials-scopus",title:"Scopus",section:"Socials",handler:()=>{window.open("https://www.scopus.com/authid/detail.uri?authorId=56742468200","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/fangyizhang-x","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/fangyi-zhang-a6108088","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>