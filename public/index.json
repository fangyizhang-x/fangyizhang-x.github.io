[{"authors":null,"categories":null,"content":"Dr. Fangyi Zhang is currently a research fellow in the QUT Centre for Robotics. He was also a former PhD student in the Australian Centre for Robotic Vision (ACRV) at QUT node, supervised by Prof. Peter Corke, Dr. J√ºrgen Leitner, Prof. Michael Milford and Dr. Ben Upcroft. His PhD research was focused on Deep Reinforcement Learning and Transfer Learning for Robotic Reaching, and obtained the degree in 2018 with the thesis entitled \u0026ldquo;Learning Real-world Visuo-motor Policies from Simulation\u0026rdquo;. After that, he joined Alibaba DAMO Academy as a Research Scientist, doing research and development work on drone applications and data mining. His current research interests include robot learning, robotic vision, robotic manipulation, and autonomous systems.\nPrior to his PhD, Fangyi obtained his B.Eng. degree in Automation from the East China Jiaotong University in 2010, followed with three years' work experience on R\u0026amp;D of locomotive control algorithms and electrical systems in the CRRC Zhuzhou Institute from 2010 to 2013. In 2014, he stayed for one year at the Hong Kong University of Science and Technology, as a research assistant supervised by Prof. Ming Liu, doing research on VLC-based indoor localization and 2D-laser based 3D sensing.\n Download my resum√©. -- ","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Dr. Fangyi Zhang is currently a research fellow in the QUT Centre for Robotics. He was also a former PhD student in the Australian Centre for Robotic Vision (ACRV) at QUT node, supervised by Prof.","tags":null,"title":"Fangyi Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"14776e09e17b9d3bca37582387deb4b4","permalink":"https://new.fangyizhang.com/home_not_used/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home_not_used/skills/","section":"home_not_used","summary":"","tags":null,"title":"Skills","type":"home_not_used"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d54a79a6e1bc7f6d77a4bd54127910cf","permalink":"https://new.fangyizhang.com/home_not_used/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home_not_used/experience/","section":"home_not_used","summary":"","tags":null,"title":"Experience","type":"home_not_used"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"048d56083ea83447606e6ff57e434f9c","permalink":"https://new.fangyizhang.com/home_not_used/accomplishments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home_not_used/accomplishments/","section":"home_not_used","summary":"","tags":null,"title":"Accomplish\u0026shy;ments","type":"home_not_used"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d1c18bad73735ad8f4e0654ac4b40422","permalink":"https://new.fangyizhang.com/home_not_used/posts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home_not_used/posts/","section":"home_not_used","summary":"","tags":null,"title":"Recent Posts","type":"home_not_used"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b1d08d2fca1d3ca88915bc9ed2b32ddb","permalink":"https://new.fangyizhang.com/home_not_used/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home_not_used/talks/","section":"home_not_used","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"home_not_used"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e7b76de46cfb2992ba6c97b5e8d53fbe","permalink":"https://new.fangyizhang.com/home_not_used/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home_not_used/featured/","section":"home_not_used","summary":"","tags":null,"title":"Featured Publications","type":"home_not_used"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3b5b7a733405d26d8c2c2ca31cfe89cd","permalink":"https://new.fangyizhang.com/home_not_used/tags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home_not_used/tags/","section":"home_not_used","summary":"","tags":null,"title":"Popular Topics","type":"home_not_used"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://new.fangyizhang.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Fangyi Zhang","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://new.fangyizhang.com/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["**Fangyi Zhang**","J√ºrgen Leitner","Zongyuan Ge","Michael Milford","Peter Corke"],"categories":null,"content":"   ","date":1566172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566172800,"objectID":"1614f96d6207da64dc8a83c48c7acbe0","permalink":"https://new.fangyizhang.com/publication/adt_ijrr/","publishdate":"2019-08-19T00:00:00Z","relpermalink":"/publication/adt_ijrr/","section":"publication","summary":"Various approaches have been proposed to learn visuo-motor policies for real-world robotic applications. One solution is first learning in simulation then transferring to the real world. In the transfer, most existing approaches need real-world images with labels. However, the labeling process is often expensive or even impractical in many robotic applications. In this article, we introduce an adversarial discriminative sim-to-real transfer approach to reduce the amount of labeled real data required. The effectiveness of the approach is demonstrated with modular networks in a table-top object-reaching task where a seven-degree-of-freedom arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations from a monocular RGB camera. The adversarial transfer approach reduced the labeled real data requirement by 50%. Policies can be transferred to real environments with only 93 labeled and 186 unlabeled real images. The transferred visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 97.8% success rate and 1.8 cm control accuracy. Datasets and code are openly available.","tags":null,"title":"Adversarial Discriminative Sim-to-real Transfer of Visuo-motor Policies","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://new.fangyizhang.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"This is my Ph.D. project in the Australian Centre for Robotic Vision at QUT, with supervisions from Prof. Peter Corke, Dr. J√ºrgen Leitner, Prof. Michael Milford and Dr. Ben Upcroft.\nLearning Planar Reaching in Simulation   Robotic Planar Reaching in the Real World   Learning Table-top Object Reaching with a 7 DoF Robotic Arm from Simulation   Contributions:\n Feasibility analysis on learning vision-based robotic planar reaching using DQNs in simulation. Proposed a modular deep Q network architecture for fast and low-cost transfer of visuo-motor policies from simulation to the real world. Proposed an end-to-end fine-tuning method using weighted losses to improve hand-eye coordination. Proposed a kinematics-based guided policy search method (K-GPS) to speed up Q learning for robotic applications where kinematic models are known. Demonstrated in robotic reaching tasks on a real Baxter robot in velocity and position control modes, e.g., table-top object reaching in clutter and planar reaching. More investigations are undergoing for semi-supervised and unsupervised transfer from simulation to the real world using adversarial discriminative approaches.  ","date":1527724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527724800,"objectID":"d9612d08befe8ceac6a7261941db9855","permalink":"https://new.fangyizhang.com/project/sim2real_learning/","publishdate":"2018-05-31T00:00:00Z","relpermalink":"/project/sim2real_learning/","section":"project","summary":"This is my Ph.D. project in the Australian Centre for Robotic Vision at QUT, with supervisions from Prof. Peter Corke, Dr. J√ºrgen Leitner, Prof. Michael Milford and Dr. Ben Upcroft.","tags":["transfer learning","sim-to-real transfer","deep learning","reinforcement learning","visuo-motor policy","robotic reaching"],"title":"Learning Real-world Visuo-motor Policies from Simulation","type":"project"},{"authors":["**Fangyi Zhang**","J√ºrgen Leitner","Michael Milford","Peter Corke"],"categories":null,"content":"   ","date":1512950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512950400,"objectID":"9711decdf7c5149d55549eb7dc67b9ee","permalink":"https://new.fangyizhang.com/publication/acra2017/","publishdate":"2017-12-11T00:00:00Z","relpermalink":"/publication/acra2017/","section":"publication","summary":"While deep learning has had significant successes in computer vision thanks to the abundance of visual data, collecting sufficiently large real-world datasets for robot learning can be costly. To increase the practicality of these techniques on real robots, we propose a modular deep reinforcement learning method capable of transferring models trained in simulation to a real-world robotic task. We introduce a bottleneck between perception and control, enabling the networks to be trained independently, but then merged and fine-tuned in an end-to-end manner. On a canonical, planar visually-guided robot reaching task a fine-tuned accuracy of 1.6 pixels is achieved, a significant improvement over naive transfer (17.5 px), showing the potential for more complicated and broader applications. Our method provides a technique for more efficiently improving hand-eye coordination on a real robotic system without relying entirely on large real-world robot datasets.","tags":null,"title":"Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies","type":"publication"},{"authors":["**Fangyi Zhang**","J√ºrgen Leitner","Michael Milford","Peter Corke"],"categories":null,"content":"   ","date":1500595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500595200,"objectID":"c2f935695dee513d9b4f80428865e47c","permalink":"https://new.fangyizhang.com/publication/cvpr2017/","publishdate":"2017-07-21T00:00:00Z","relpermalink":"/publication/cvpr2017/","section":"publication","summary":"This paper introduces an end-to-end fine-tuning method to improve hand-eye coordination in modular deep visuo-motor policies (modular networks) where each module is trained independently. Benefiting from weighted losses, the fine-tuning method significantly improves the performance of the policies for a robotic planar reaching task.","tags":null,"title":"Tuning Modular Networks with Weighted Losses for Hand-Eye Coordination","type":"publication"},{"authors":["J√ºrgen Leitner"," Adam W. Tow","Niko S√ºnderhauf","Jake E. Dean","Joseph W. Durham","Matthew Cooper","Markus Eich","Christopher Lehnert","Ruben Mangels","Christopher McCool","Peter T. Kujala","Lachlan Nicholson","Trung Pham","James Sergeant","Liao Wu","**Fangyi Zhang**","Ben Upcroft","Peter Corke"],"categories":null,"content":"   ","date":1496016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496016000,"objectID":"38288a7a1340034e0ddedca2555181e2","permalink":"https://new.fangyizhang.com/publication/icra2017/","publishdate":"2017-05-29T00:00:00Z","relpermalink":"/publication/icra2017/","section":"publication","summary":"Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA Challenges are an established and important way to drive scientific progress. They make research comparable on a well-defined benchmark with equal test conditions for all participants. However, such challenge events occur only occasionally, are limited to a small number of contestants, and the test conditions are very difficult to replicate after the main event. We present a new physical benchmark challenge for robotic picking: the ACRV Picking Benchmark. Designed to be reproducible, it consists of a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. A well-defined evaluation protocol enables the comparison of complete robotic systems - including perception and manipulation - instead of sub-systems only. Our paper also describes and reports results achieved by an open baseline system based on a Baxter robot.","tags":null,"title":"The ACRV Picking Benchmark: A Robotic Shelf Picking Benchmark to Foster Reproducible Research","type":"publication"},{"authors":null,"categories":null,"content":"  Amazon Picking Challenge (2016)   As part of the Team ACRV for the Amazon Picking Challenge 2016, I worked on hand-eye calibration with Dr. Leo Wu.     Household Applications   This is a project I worked on during my visit to the Perception and Robotics Group at the University of Maryland, College Park, Sep-Dec 2016. A mobile manipulation robot was developed for housework in a kitchen scenario, where I mainly took charge of the sub-task of table cleaning. Thanks to Prof. Yiannis Aloimonos, Dr. Cornelia Ferm√ºller, Dr. Yi Li, Dr. Yezhou Yang and my colleagues: Dr. Fang Wang, Dr. Ren Mao, Dr. Aleksandrs Ecins, Dr. Wentao Luan, Mr. Konstantinos Zampogiannis, Mr. Yi Zhang, Mr. Kanishka Ganguly, Mr. Chethan M Parameshwara.\nTable Cleaning   Refrigerator Operating   Microwave Operating   ","date":1483142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483142400,"objectID":"8734a4b7a72921cfea3b7800b7ab09b5","permalink":"https://new.fangyizhang.com/project/robotic_manipulation/","publishdate":"2016-12-31T00:00:00Z","relpermalink":"/project/robotic_manipulation/","section":"project","summary":"Amazon Picking Challenge (2016)   As part of the Team ACRV for the Amazon Picking Challenge 2016, I worked on hand-eye calibration with Dr. Leo Wu.     Household Applications   This is a project I worked on during my visit to the Perception and Robotics Group at the University of Maryland, College Park, Sep-Dec 2016.","tags":["robotic manipulation","robotic picking and placing","robotic vision","deep learning","motion planning"],"title":"Robotic Manipulation for Warehouse and Household Applications","type":"project"},{"authors":["Kejie Qiu","**Fangyi Zhang**","Ming Liu"],"categories":null,"content":"  -- ","date":1478476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478476800,"objectID":"cd5b06878eca5fe9463f424bb07f1b88","permalink":"https://new.fangyizhang.com/publication/ram2016/","publishdate":"2016-11-07T00:00:00Z","relpermalink":"/publication/ram2016/","section":"publication","summary":"We propose to use visible-light beacons for low-cost indoor localization. Modulated light-emitting diode (LED) lights are adapted for localization as well as illumination. The proposed solution consists of two components: light-signal decomposition and Bayesian localization.","tags":null,"title":"Let the Light Guide Us: VLC-based Localization","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://new.fangyizhang.com/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["**Fangyi Zhang**","J√ºrgen Leitner","Michael Milford","Ben Upcroft","Peter Corke"],"categories":null,"content":"   ","date":1449014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1449014400,"objectID":"d60096470bdda31fbccb8bc53e3c3d12","permalink":"https://new.fangyizhang.com/publication/acra2015/","publishdate":"2015-12-02T00:00:00Z","relpermalink":"/publication/acra2015/","section":"publication","summary":"This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.","tags":null,"title":"Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control","type":"publication"},{"authors":["Kejie Qiu","**Fangyi Zhang**","Ming Liu"],"categories":null,"content":"  -- ","date":1443398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443398400,"objectID":"b711ac02accd5d5e006fc1cc7560fef0","permalink":"https://new.fangyizhang.com/publication/iros2015/","publishdate":"2015-09-28T00:00:00Z","relpermalink":"/publication/iros2015/","section":"publication","summary":"For mobile robots and position-based services, such as healthcare service, precise localization is the most fundamental capability while low-cost localization solutions are with increasing need and potentially have a wide market. A low-cost localization solution based on a novel Visible Light Communication (VLC) system for indoor environments is proposed in this paper. A number of modulated LED lights are used as beacons to aid indoor localization additional to illumination. A Gaussian Process(GP) is used to model the intensity distributions of the light sources. A Bayesian localization framework is constructed using the results of the GP, leading to precise localization. Path-planning is hereby feasible by only using the GP variance field, rather than using a metric map. Dijkstra's algorithm-based path-planner is adopted to cope with the practical situations. We demonstrate our localization system by real-time experiments performed on a tablet PC in an indoor environment.","tags":null,"title":"Visible Light Communication-based Indoor Localization using Gaussian Process","type":"publication"},{"authors":["Kejie Qiu","**Fangyi Zhang**","Ming Liu"],"categories":null,"content":"  -- ","date":1440374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440374400,"objectID":"0ce0ba195f4f044c04ea3d2e04711c23","permalink":"https://new.fangyizhang.com/publication/case2015/","publishdate":"2015-08-24T00:00:00Z","relpermalink":"/publication/case2015/","section":"publication","summary":"For mobile robots and position-based services, localization is the most fundamental capability while path-planning is an important application based on that. A novel localization and path-planning solution based on a low-cost Visible Light Communication (VLC) system for indoor environments is proposed in this paper. A number of modulated LED lights are used as beacons to aid indoor localization additional to illumination. A Gaussian Process (GP) is used to model the intensity distributions of the light sources. Path-planning is hereby feasible by using the GP variance field, rather than using a metric map. Graph-based path-planners are introduced to cope with the practical situations. We demonstrate our path-planning system by real-time experiments performed on a tablet PC in an indoor environment.","tags":null,"title":"Visible Light Communication-based Indoor Environment Modeling and Metric-free Path Planning","type":"publication"},{"authors":["**Fangyi Zhang**","Kejie Qiu","Ming Liu"],"categories":null,"content":"   ","date":1432598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1432598400,"objectID":"0b78ab5f84d8388ac247b548ffc6aa8d","permalink":"https://new.fangyizhang.com/publication/icra2015/","publishdate":"2015-05-26T00:00:00Z","relpermalink":"/publication/icra2015/","section":"publication","summary":"Indoor localization is a fundamental capability for service robots and indoor applications on mobile devices. To realize that, the cost and performance are of great concern. In this paper, we introduce a lightweight signal encoding and decomposition method for a low-cost and low-power Visible Light Communication (VLC)-based indoor localization system. Firstly, a Gold-sequence-based tiny-length code selection method is introduced for light encoding. Then a correlation-based asynchronous blind light-signal decomposition method is developed for the decomposition of the lights mixed with modulated light sources. It is able to decompose the mixed light-signal package in real-time. The average decomposition time-cost for each frame is 20 ms. By using the decomposition results, the localization system achieves accuracy at 0.56 m. These features outperform other existing low-cost indoor localization approaches, such as WiFiSLAM.","tags":null,"title":"Asynchronous Blind Signal Decomposition Using Tiny-Length Code for Visible Light Communication-Based Indoor Localization","type":"publication"},{"authors":null,"categories":null,"content":"This is a project I worked on with Prof. Ming Liu and Mr. Kejie Qiu when I was a research assistant in the RAM-LAB at HKUST.\n  Contributions:\n Developed a beacon code selection algorithm and a decomposition algorithm for blindly mixed beacon signals, based on CDMA code selection principles and Gold-sequence correlation properties. Participated in the development of a light intensity distribution map generation algorithm using Gaussian Process Regression. Participated in the development of localization and path planning algorithms using Kalman Filter (KF) and A star.  ","date":1419984000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1419984000,"objectID":"d21ddde87b726a2ba9328c8c9cd6aaa0","permalink":"https://new.fangyizhang.com/project/vlc/","publishdate":"2014-12-31T00:00:00Z","relpermalink":"/project/vlc/","section":"project","summary":"This is a project I worked on with Prof. Ming Liu and Mr. Kejie Qiu when I was a research assistant in the RAM-LAB at HKUST.\n  Contributions:\n Developed a beacon code selection algorithm and a decomposition algorithm for blindly mixed beacon signals, based on CDMA code selection principles and Gold-sequence correlation properties.","tags":["indoor localization","visible light communication","path planning"],"title":"VLC-Based Indoor Localization","type":"project"},{"authors":["Fangyi Zhang","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://new.fangyizhang.com/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://new.fangyizhang.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]