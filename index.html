<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Fangyi Zhang </title> <meta name="author" content="Fangyi Zhang"> <meta name="description" content="Fangyi Zhang's personal website. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="robotics, AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/robot_x.png?350742e6cec5788ce59ac83f990524b0"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.fangyizhang.com/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%64%72.%66%61%6E%67%79%69.%7A%68%61%6E%67@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-3938-5377" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=5jFI06UAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Fangyi_Zhang/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=56742468200" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://github.com/fangyizhang-x" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/fangyi-zhang-a6108088" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Fangyi</span> Zhang </h1> <p class="desc">Research Fellow, <a href="https://research.qut.edu.au/qcr/" rel="external nofollow noopener" target="_blank">QUT Centre for Robotics</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cv_pic-480.webp 480w,/assets/img/cv_pic-800.webp 800w,/assets/img/cv_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/cv_pic.jpg?121ff0133f9e770fba0a5122a3b8fb69" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="cv_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Since 2014, Dr. Fangyi Zhang has specialized in robotics and machine learning, establishing a robust record of publications and demonstrating leadership in both research and engagement. He has authored over 20 peer-reviewed publications, including contributions to top conferences and journals in robotics (IJRR, RA-L, ICRA, IROS) and machine learning (NeurIPS, ICLR). His academic excellence has been recognized with several awards, including a Best Paper Award Finalist at ACRA 2017 and a Best Industry Paper at IJCAIW 2021.</p> <p>Dr. Fangyi Zhang is currently a research fellow in the <a href="https://research.qut.edu.au/qcr/" rel="external nofollow noopener" target="_blank">QUT Centre for Robotics</a>. He completed his PhD at the <a href="https://www.roboticvision.org/" rel="external nofollow noopener" target="_blank">Australian Centre for Robotic Vision (ACRV)</a> at QUT node, under the supervision of <a href="https://wiki.qut.edu.au/display/cyphy/Peter+Corke" rel="external nofollow noopener" target="_blank">Prof. Peter Corke</a>, <a href="http://juxi.net/" rel="external nofollow noopener" target="_blank">Dr. Jürgen Leitner</a>, and <a href="https://wiki.qut.edu.au/display/cyphy/Michael+Milford" rel="external nofollow noopener" target="_blank">Prof. Michael Milford</a>. His PhD research, which he completed in 2018, focused on Deep Reinforcement Learning and Transfer Learning for Robotic Reaching, and his thesis was titled <a href="https://eprints.qut.edu.au/121471/" rel="external nofollow noopener" target="_blank">“Learning Real-world Visuo-motor Policies from Simulation”</a>. Following his PhD, he joined <a href="https://damo.alibaba.com/" rel="external nofollow noopener" target="_blank">Alibaba DAMO Academy</a> as an Algorithm Expert, where he worked on drone applications and data mining. His current research interests include robot learning, tactile sensing, robotic vision, robotic manipulation, and autonomous systems.</p> <p>Before pursuing his PhD, Dr. Zhang earned his B.Eng. degree in Automation from East China Jiaotong University in 2010. He then spent three years working on R&amp;D of locomotive control algorithms and electrical systems at the <a href="https://www.crrcgc.cc/zzsen/47_2774/47_2807/index.html" rel="external nofollow noopener" target="_blank">CRRC Zhuzhou Institute</a> from 2010 to 2013. In 2014, he was a research assistant at the Hong Kong University of Science and Technology, supervised by <a href="https://ram-lab.com/people/#dr-ming-liu-director" rel="external nofollow noopener" target="_blank">Prof. Ming Liu</a>, where he conducted research on VLC-based indoor localization and 2D-laser based 3D sensing.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Aug 15, 2024</th> <td> Our research on <a href="https://github.com/fangyizhang-x/inst-obj" rel="external nofollow noopener" target="_blank">“assessing robotic grasping through instrumented objects”</a> has been featured by QUT News: <a href="https://www.qut.edu.au/news?id=196009" rel="external nofollow noopener" target="_blank">“A little help for robots that don’t know their own strength”</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 17, 2024</th> <td> Our RA-L paper <a href="https://github.com/fangyizhang-x/inst-obj" rel="external nofollow noopener" target="_blank">“Towards assessing compliant robotic grasping from first-object perspective via instrumented objects”</a> will be presented at the 40th Anniversary of the IEEE Conference on Robotics and Automation (<a href="https://icra40.ieee.org/" rel="external nofollow noopener" target="_blank">“ICRA@40”</a>). </td> </tr> <tr> <th scope="row" style="width: 20%">May 17, 2024</th> <td> Our <a href="https://deformable-workshop.github.io/icra2024/" rel="external nofollow noopener" target="_blank">4th Workshop on Representing and Manipulating Deformable Objects</a> will be held at ICRA 2024 on the 17th of May in PACIFICO Yokohama North Room: G304. </td> </tr> <tr> <th scope="row" style="width: 20%">May 16, 2024</th> <td> Our research on <a href="https://sites.google.com/view/foldingbyhand" rel="external nofollow noopener" target="_blank">“learning fabric manipulation in the real world with human videos”</a> was presented at <a href="https://2024.ieee-icra.org/" rel="external nofollow noopener" target="_blank">“ICRA 2024”</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">May 02, 2024</th> <td> Our paper <a href="https://github.com/fangyizhang-x/inst-obj" rel="external nofollow noopener" target="_blank">“Towards assessing compliant robotic grasping from first-object perspective via instrumented objects”</a> got accepted to RA-L. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Towards_Assessing-480.webp 480w,/assets/img/publication_preview/Towards_Assessing-800.webp 800w,/assets/img/publication_preview/Towards_Assessing-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Towards_Assessing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Towards_Assessing.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="z10538419" class="col-sm-8"> <div class="title">Towards Assessing Compliant Robotic Grasping From First-Object Perspective via Instrumented Objects</div> <div class="author"> Maceon Knopke, Liguo Zhu, Peter Corke, and <em>Fangyi Zhang</em> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/fangyizhang-x/inst-obj" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/abstract/document/10538419" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=kQSZlNxYRrs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Grasping compliant objects is difficult for robots - applying too little force may cause the grasp to fail, while too much force may lead to object damage. A robot needs to apply the right amount of force to quickly and confidently grasp the objects so that it can perform the required task. Although some methods have been proposed to tackle this issue, performance assessment is still a problem for directly measuring object property changes and possible damage. To fill the gap, a new concept is introduced in this paper to assess compliant robotic grasping using instrumented objects. A proof-of-concept design is proposed to measure the force applied on a cuboid object from a first-object perspective. The design can detect multiple contact locations and applied forces on its surface by using multiple embedded 3D Hall sensors to detect deformation relative to embedded magnets. The contact estimation is achieved by interpreting the Hall-effect signals using neural networks. In comprehensive experiments, the design achieved good performance in estimating contacts from each single face of the cuboid and decent performance in detecting contacts from multiple faces when being used to evaluate grasping from a parallel jaw gripper, demonstrating the effectiveness of the design and the feasibility of the concept.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/learning_fabric-480.webp 480w,/assets/img/publication_preview/learning_fabric-800.webp 800w,/assets/img/publication_preview/learning_fabric-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/learning_fabric.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="learning_fabric.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="lee2022learning" class="col-sm-8"> <div class="title">Learning fabric manipulation in the real world with human videos</div> <div class="author"> Robert Lee, Jad Abou-Chakra, <em>Fangyi Zhang</em>, and Peter Corke </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://sites.google.com/view/foldingbyhand" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/document/10610062" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Fabric manipulation is a long-standing challenge in robotics due to the enormous state space and complex dynamics. Learning approaches stand out as promising for this domain as they allow us to learn behaviours directly from data. Most prior methods however rely heavily on simulation, which is still limited by the large sim-to-real gap of deformable objects or rely on large datasets. A promising alternative is to learn fabric manipulation directly from watching humans perform the task. In this work, we explore how demonstrations for fabric manipulation tasks can be collected directly by humans, providing an extremely natural and fast data collection pipeline. Then, using only a handful of such demonstrations, we show how a pick-and-place policy can be learned and deployed on a real robot, without any robot data collection at all. We demonstrate our approach on a fabric smoothing and folding task, showing that our policy can reliably reach folded states from crumpled initial configurations. Code, video and data are available on the project website: https://sites.google.com/view/foldingbyhand</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/re_evaluate_tactile-480.webp 480w,/assets/img/publication_preview/re_evaluate_tactile-800.webp 800w,/assets/img/publication_preview/re_evaluate_tactile-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/re_evaluate_tactile.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="re_evaluate_tactile.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="z10342262" class="col-sm-8"> <div class="title">Re-Evaluating Parallel Finger-Tip Tactile Sensing for Inferring Object Adjectives: An Empirical Study</div> <div class="author"> <em>Fangyi Zhang</em>, and Peter Corke </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/10342262" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Finger-tip tactile sensors are increasingly used for robotic sensing to establish stable grasps and to infer object properties. Promising performance has been shown in a number of works for inferring adjectives that describe the object, but there remains a question about how each taxel contributes to the performance. This paper explores this question with empirical experiments, leading insights for future finger-tip tactile sensor usage and design.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Robust_Graph_Structure-480.webp 480w,/assets/img/publication_preview/Robust_Graph_Structure-800.webp 800w,/assets/img/publication_preview/Robust_Graph_Structure-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Robust_Graph_Structure.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Robust_Graph_Structure.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wang2022robust" class="col-sm-8"> <div class="title">Robust Graph Structure Learning via Multiple Statistical Tests</div> <div class="author"> Yaohua Wang, <em>Fangyi Zhang</em>, Ming Lin, Senzhang Wang, Xiuyu Sun, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Rong Jin' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/cf7700139af1fa346d2f57f1f5c26c18-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Thomas-wyh/B-Attention" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Graph structure learning aims to learn connectivity in a graph from data. It is particularly important for many computer vision related tasks since no explicit graph structure is available for images for most cases. A natural way to construct a graph among images is to treat each image as a node and assign pairwise image similarities as weights to corresponding edges. It is well known that pairwise similarities between images are sensitive to the noise in feature representations, leading to unreliable graph structures. We address this problem from the viewpoint of statistical tests. By viewing the feature vector of each node as an independent sample, the decision of whether creating an edge between two nodes based on their similarity in feature representation can be thought as a single statistical test. To improve the robustness in the decision of creating an edge, multiple samples are drawn and integrated by multiple statistical tests to generate a more reliable similarity measure, consequentially more reliable graph structure. The corresponding elegant matrix form named -Attention is designed for efficiency. The effectiveness of multiple tests for graph structure learning is verified both theoretically and empirically on multiple clustering and ReID benchmark datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Face_Clustering_via-480.webp 480w,/assets/img/publication_preview/Face_Clustering_via-800.webp 800w,/assets/img/publication_preview/Face_Clustering_via-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Face_Clustering_via.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Face_Clustering_via.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wang2022adanets" class="col-sm-8"> <div class="title">Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space</div> <div class="author"> Yaohua Wang, Yaobin Zhang, <em>Fangyi Zhang</em>, Senzhang Wang, Ming Lin, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Yuqi Zhang, Xiuyu Sun' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2202.03800" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Face clustering has attracted rising research interest recently to take advantage of massive amounts of face images on the web. State-of-the-art performance has been achieved by Graph Convolutional Networks (GCN) due to their powerful representation capacity. However, existing GCN-based methods build face graphs mainly according to kNN relations in the feature space, which may lead to a lot of noise edges connecting two faces of different classes. The face features will be polluted when messages pass along these noise edges, thus degrading the performance of GCNs. In this paper, a novel algorithm named Ada-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In Ada-NETS, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCNs to cluster faces. Experiments on multiple public clustering datasets show that Ada-NETS significantly outperforms current state-of-the-art methods, proving its superiority and generalization.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Adversarial_discriminative-480.webp 480w,/assets/img/publication_preview/Adversarial_discriminative-800.webp 800w,/assets/img/publication_preview/Adversarial_discriminative-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/Adversarial_discriminative.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Adversarial_discriminative.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2019adversarial" class="col-sm-8"> <div class="title">Adversarial discriminative sim-to-real transfer of visuo-motor policies</div> <div class="author"> <em>Fangyi Zhang</em>, Jürgen Leitner, Zongyuan Ge, Michael Milford, and Peter Corke </div> <div class="periodical"> <em>The International Journal of Robotics Research (IJRR)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1709.05746" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Various approaches have been proposed to learn visuo-motor policies for real-world robotic applications. One solution is first learning in simulation then transferring to the real world. In the transfer, most existing approaches need real-world images with labels. However, the labelling process is often expensive or even impractical in many robotic applications. In this paper, we propose an adversarial discriminative sim-to-real transfer approach to reduce the cost of labelling real data. The effectiveness of the approach is demonstrated with modular networks in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The adversarial transfer approach reduced the labelled real data requirement by 50%. Policies can be transferred to real environments with only 93 labelled and 186 unlabelled real images. The transferred visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 97.8% success rate and 1.8 cm control accuracy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/The_ACRV_picking-480.webp 480w,/assets/img/publication_preview/The_ACRV_picking-800.webp 800w,/assets/img/publication_preview/The_ACRV_picking-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/The_ACRV_picking.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="The_ACRV_picking.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="leitner2017acrv" class="col-sm-8"> <div class="title">The ACRV picking benchmark: A robotic shelf picking benchmark to foster reproducible research</div> <div class="author"> Jürgen Leitner, Adam W. Tow, Niko Sünderhauf, Jake E. Dean, Joseph W. Durham, and <span class="more-authors" title="click to view 13 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '13 more authors' ? 'Matthew Cooper, Markus Eich, Christopher Lehnert, Ruben Mangels, Christopher McCool, Peter T. Kujala, Lachlan Nicholson, Trung Pham, James Sergeant, Liao Wu, Fangyi Zhang, Ben Upcroft, Peter Corke' : '13 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">13 more authors</span> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1609.05258" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA Challenges are an established and important way to drive scientific progress. They make research comparable on a well-defined benchmark with equal test conditions for all participants. However, such challenge events occur only occasionally, are limited to a small number of contestants, and the test conditions are very difficult to replicate after the main event. We present a new physical benchmark challenge for robotic picking: the ACRV Picking Benchmark (APB). Designed to be reproducible, it consists of a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. A well-defined evaluation protocol enables the comparison of \emphcomplete robotic systems – including perception and manipulation – instead of sub-systems only. Our paper also describes and reports results achieved by an open baseline system based on a Baxter robot.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%64%72.%66%61%6E%67%79%69.%7A%68%61%6E%67@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-3938-5377" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=5jFI06UAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Fangyi_Zhang/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://www.scopus.com/authid/detail.uri?authorId=56742468200" title="Scopus" rel="external nofollow noopener" target="_blank"><i class="ai ai-scopus"></i></a> <a href="https://github.com/fangyizhang-x" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/fangyi-zhang-a6108088" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <div class="contact-note">Please reach me via email of any interest. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Fangyi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1Y8LRS1FDK"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1Y8LRS1FDK");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"Fangyi Zhang&#39;s CV.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/05/01/tabs.html"}},{id:"news-our-paper-re-evaluating-parallel-finger-tip-tactile-sensing-for-inferring-object-adjectives-an-empirical-study-was-presented-at-iros-2023",title:"Our paper \u201cRe-Evaluating Parallel Finger-Tip Tactile Sensing for Inferring Object Adjectives: An Empirical...",description:"",section:"News"},{id:"news-our-paper-towards-assessing-compliant-robotic-grasping-from-first-object-perspective-via-instrumented-objects-got-accepted-to-ra-l",title:"Our paper \u201cTowards assessing compliant robotic grasping from first-object perspective via instrumented objects\u201d...",description:"",section:"News"},{id:"news-our-research-on-learning-fabric-manipulation-in-the-real-world-with-human-videos-was-presented-at-icra-2024",title:"Our research on \u201clearning fabric manipulation in the real world with human videos\u201d...",description:"",section:"News"},{id:"news-our-4th-workshop-on-representing-and-manipulating-deformable-objects-will-be-held-at-icra-2024-on-the-17th-of-may-in-pacifico-yokohama-north-room-g304",title:"Our 4th Workshop on Representing and Manipulating Deformable Objects will be held at...",description:"",section:"News"},{id:"news-our-ra-l-paper-towards-assessing-compliant-robotic-grasping-from-first-object-perspective-via-instrumented-objects-will-be-presented-at-the-40th-anniversary-of-the-ieee-conference-on-robotics-and-automation-icra-40",title:"Our RA-L paper \u201cTowards assessing compliant robotic grasping from first-object perspective via instrumented...",description:"",section:"News"},{id:"news-our-research-on-assessing-robotic-grasping-through-instrumented-objects-has-been-featured-by-qut-news-a-little-help-for-robots-that-don-t-know-their-own-strength",title:"Our research on \u201cassessing robotic grasping through instrumented objects\u201d has been featured by...",description:"",section:"News"},{id:"projects-learning-real-world-visuo-motor-policies-from-simulation",title:"Learning Real-world Visuo-motor Policies from Simulation",description:"This is my Ph.D. project in the Australian Centre for Robotic Vision at QUT, with supervisions from Prof. Peter Corke, Dr. J\xfcrgen Leitner, Prof. Michael Milford and Dr. Ben Upcroft.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-robotic-manipulation-for-warehouse-and-household-applications",title:"Robotic Manipulation for Warehouse and Household Applications",description:"Amazon Picking Challenge (2016) As part of the Team ACRV for the Amazon Picking Challenge 2016, I worked on hand-eye calibration with Dr. Leo Wu. Household Applications This is a project I worked on during my visit to the Perception and Robotics Group at the University of Maryland, College Park, Sep-Dec 2016.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-vlc-based-indoor-localization",title:"VLC-Based Indoor Localization",description:"This is a project I worked on with Prof. Ming Liu and Mr. Kejie Qiu when I was a research assistant in the RAM-LAB at HKUST. The project developed a beacon code selection algorithm and a decomposition algorithm for blindly mixed beacon signals, based on CDMA code selection principles and Gold-sequence correlation properties.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-tactile-based-robotic-manipulation",title:"Tactile-based Robotic Manipulation",description:"A project exploring tactile sensing and tactile-based robotic manipulation.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-assessing-compliant-robotic-manipulation-with-instrumented-objects",title:"Assessing Compliant Robotic Manipulation with Instrumented Objects",description:"A project investigating using instrumented objects to assess compliant robotic manipulation performance.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%64%72.%66%61%6E%67%79%69.%7A%68%61%6E%67@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0003-3938-5377","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=5jFI06UAAAAJ","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Fangyi_Zhang/","_blank")}},{id:"socials-scopus",title:"Scopus",section:"Socials",handler:()=>{window.open("https://www.scopus.com/authid/detail.uri?authorId=56742468200","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/fangyizhang-x","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/fangyi-zhang-a6108088","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>