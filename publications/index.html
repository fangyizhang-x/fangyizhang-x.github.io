<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Fangyi Zhang </title> <meta name="author" content="Fangyi Zhang"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="robotics, AI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/robot_x.png?350742e6cec5788ce59ac83f990524b0"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.fangyizhang.com/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Fangyi</span> Zhang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <article> <script src="/assets/js/bibsearch.js?a8796296a5e2f80c5e498cdd51d7761e" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Towards_Assessing-480.webp 480w,/assets/img/publication_preview/Towards_Assessing-800.webp 800w,/assets/img/publication_preview/Towards_Assessing-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Towards_Assessing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Towards_Assessing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10538419" class="col-sm-8"> <div class="title">Towards Assessing Compliant Robotic Grasping From First-Object Perspective via Instrumented Objects</div> <div class="author"> Maceon Knopke, Liguo Zhu, Peter Corke, and <em>Fangyi Zhang</em> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/10538419" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Grasping compliant objects is difficult for robots - applying too little force may cause the grasp to fail, while too much force may lead to object damage. A robot needs to apply the right amount of force to quickly and confidently grasp the objects so that it can perform the required task. Although some methods have been proposed to tackle this issue, performance assessment is still a problem for directly measuring object property changes and possible damage. To fill the gap, a new concept is introduced in this paper to assess compliant robotic grasping using instrumented objects. A proof-of-concept design is proposed to measure the force applied on a cuboid object from a first-object perspective. The design can detect multiple contact locations and applied forces on its surface by using multiple embedded 3D Hall sensors to detect deformation relative to embedded magnets. The contact estimation is achieved by interpreting the Hall-effect signals using neural networks. In comprehensive experiments, the design achieved good performance in estimating contacts from each single face of the cuboid and decent performance in detecting contacts from multiple faces when being used to evaluate grasping from a parallel jaw gripper, demonstrating the effectiveness of the design and the feasibility of the concept.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/learning_fabric-480.webp 480w,/assets/img/publication_preview/learning_fabric-800.webp 800w,/assets/img/publication_preview/learning_fabric-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/learning_fabric.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="learning_fabric.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lee2022learning" class="col-sm-8"> <div class="title">Learning fabric manipulation in the real world with human videos</div> <div class="author"> Robert Lee, Jad Abou-Chakra, <em>Fangyi Zhang</em>, and Peter Corke </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Fabric manipulation is a long-standing challenge in robotics due to the enormous state space and complex dynamics. Learning approaches stand out as promising for this domain as they allow us to learn behaviours directly from data. Most prior methods however rely heavily on simulation, which is still limited by the large sim-to-real gap of deformable objects or rely on large datasets. A promising alternative is to learn fabric manipulation directly from watching humans perform the task. In this work, we explore how demonstrations for fabric manipulation tasks can be collected directly by humans, providing an extremely natural and fast data collection pipeline. Then, using only a handful of such demonstrations, we show how a pick-and-place policy can be learned and deployed on a real robot, without any robot data collection at all. We demonstrate our approach on a fabric smoothing and folding task, showing that our policy can reliably reach folded states from crumpled initial configurations. Code, video and data are available on the project website: https://sites.google.com/view/foldingbyhand</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/optical_dnns-480.webp 480w,/assets/img/publication_preview/optical_dnns-800.webp 800w,/assets/img/publication_preview/optical_dnns-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/optical_dnns.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="optical_dnns.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="doi:10.1021/acsaelm.4c00116" class="col-sm-8"> <div class="title">All-Optical Diffractive Deep Neural Networks Enabled Laser-Reduced Graphene Oxide Tactile Sensor for Braille Recognition</div> <div class="author"> Xing Liu, Li Fang, <em>Fangyi Zhang</em>, Qiwen Zhang, Zhengfen Wan, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Xi Chen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>ACS Applied Electronic Materials</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/graphene_applications-480.webp 480w,/assets/img/publication_preview/graphene_applications-800.webp 800w,/assets/img/publication_preview/graphene_applications-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/graphene_applications.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="graphene_applications.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="https://doi.org/10.1002/admt.202300244" class="col-sm-8"> <div class="title">Doping of Laser-Induced Graphene and Its Applications</div> <div class="author"> Qiwen Zhang, <em>Fangyi Zhang</em>, Xing Liu, Zengji Yue, Xi Chen, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Zhengfen Wan' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Advanced Materials Technologies</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Abstract Laser-induced graphene (LIG) has attracted extensive attention owing to its facile preparation of graphene and direct engraving patterns for devices. Various applications are demonstrated such as sensors, supercapacitors, electrocatalysis, batteries, antimicrobial, oil and water separation, solar cells, and heaters. In recent years, doping has been employed as a significant strategy to modulate the properties of LIG and thereby improve the performance of LIG devices. Due to the patternable manufacture, controllable morphologies, and the synergistic effect of doped atoms and graphene, the doped LIG devices exhibit a high sensitivity of sensing, pseudocapacitance performance, and biological antibacterial. This paper reviews the latest novel research progress of heteroatom and nanoparticles doped LIG in synthesis, properties, and applications. The fabrications of LIG and typical doping approaches are presented. Special attention is paid to two doping processes of LIG: the one-step laser irradiation method and the two-step laser modification consisting of deposition, drop-casting, and duplicated laser pyrolysis. Doped LIG applications with improved performance are mainly highlighted. Taking advantage of doped LIG’s properties and device performances will provide excellent opportunities for developing artificial intelligence, data storage, energy, health, and environmental applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/graphene_applications2-480.webp 480w,/assets/img/publication_preview/graphene_applications2-800.webp 800w,/assets/img/publication_preview/graphene_applications2-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/graphene_applications2.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="graphene_applications2.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="quteprints244891" class="col-sm-8"> <div class="title">Laser-scribed graphene for sensors: preparation, modification, applications, and future prospects</div> <div class="author"> Xing Liu, <em>Fangyi Zhang</em>, Qiwen Zhang, Zhengfen Wan, and Xi Chen </div> <div class="periodical"> <em>Light: Advanced Manufacturing</em>, Apr 2023 </div> <div class="periodical"> Acknowledgements: The authors acknowledge funding support from the Science and Technology Commission of Shanghai Municipality (Grant No. 21DZ1100500), Shanghai Municipal Science and Technology Major Project, and Shanghai Frontiers Science Center Program (2021-2025 No. 20). Fangyi Zhang acknowledges the continued support from the Queensland University of Technology (QUT) through the Centre for Robotics. Zhengfen Wan thanks the National Natural Science Foundation of China (Grant No. 62105206) and the China Postdoctoral Science Foundation (No. 2021M692137) for their support. Xi Chen acknowledges the support from the National Natural Science Foundation of China (Grant No. 11974247). </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>\ensuremath&lt;p\ensuremath&gt;Sensors are widely used to acquire biological and environmental information for medical diagnosis, and health and environmental monitoring. Graphene is a promising new sensor material that has been widely used in sensor fabrication in recent years. Compared with many other existing graphene preparation methods, laser-scribed graphene (LSG) is simple, low-cost, environmentally friendly, and has good conductivity and high thermal stability, making it widely used in the sensor field. This paper summarizes existing LSG methods for sensor fabrication. Primary LSG preparation methods and their variants are introduced first, followed by a summary of LSG modification methods designed explicitly for sensor fabrication. Subsequently, the applications of LSG in stress, bio, gas, temperature, and humidity sensors are summarized with a particular focus on multifunctional integrated sensors. Finally, the current challenges and prospects of LSG-based sensors are discussed.\ensuremath&lt;/p\ensuremath&gt;</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/re_evaluate_tactile-480.webp 480w,/assets/img/publication_preview/re_evaluate_tactile-800.webp 800w,/assets/img/publication_preview/re_evaluate_tactile-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/re_evaluate_tactile.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="re_evaluate_tactile.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10342262" class="col-sm-8"> <div class="title">Re-Evaluating Parallel Finger-Tip Tactile Sensing for Inferring Object Adjectives: An Empirical Study</div> <div class="author"> <em>Fangyi Zhang</em>, and Peter Corke </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/10342262" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Finger-tip tactile sensors are increasingly used for robotic sensing to establish stable grasps and to infer object properties. Promising performance has been shown in a number of works for inferring adjectives that describe the object, but there remains a question about how each taxel contributes to the performance. This paper explores this question with empirical experiments, leading insights for future finger-tip tactile sensor usage and design.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/a_linkage-based-480.webp 480w,/assets/img/publication_preview/a_linkage-based-800.webp 800w,/assets/img/publication_preview/a_linkage-based-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/a_linkage-based.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="a_linkage-based.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="doi:10.1137/1.9781611977653.ch74" class="col-sm-8"> <div class="title">A Linkage-based Doubly Imbalanced Graph Learning Framework for Face Clustering</div> <div class="author"> Huafeng Yang, Qijie Shen, Xingjian Chen, <em>Fangyi Zhang</em>, and Rong Du </div> <div class="periodical"> <em>In SIAM International Conference on Data Mining</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2107.02477" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In recent years, benefiting from the expressive power of Graph Convolutional Networks (GCNs), significant breakthroughs have been made in face clustering area. However, rare attention has been paid to GCN-based clustering on imbalanced data. Although imbalance problem has been extensively studied, the impact of imbalanced data on GCN- based linkage prediction task is quite different, which would cause problems in two aspects: imbalanced linkage labels and biased graph representations. The former is similar to that in classic image classification task, but the latter is a particular problem in GCN-based clustering via linkage prediction. Significantly biased graph representations in training can cause catastrophic over-fitting of a GCN model. To tackle these challenges, we propose a linkage-based doubly imbalanced graph learning framework for face clustering. In this framework, we evaluate the feasibility of those existing methods for imbalanced image classification problem on GCNs, and present a new method to alleviate the imbalanced labels and also augment graph representations using a Reverse-Imbalance Weighted Sampling (RIWS) strategy. With the RIWS strategy, probability-based class balancing weights could ensure the overall distribution of positive and negative samples; in addition, weighted random sampling provides diverse subgraph structures, which effectively alleviates the over-fitting problem and improves the representation ability of GCNs. Extensive experiments on series of imbalanced benchmark datasets synthesized from MS-Celeb-1M and DeepFashion demonstrate the effectiveness and generality of our proposed method.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Robust_Graph_Structure-480.webp 480w,/assets/img/publication_preview/Robust_Graph_Structure-800.webp 800w,/assets/img/publication_preview/Robust_Graph_Structure-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Robust_Graph_Structure.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Robust_Graph_Structure.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2022robust" class="col-sm-8"> <div class="title">Robust Graph Structure Learning via Multiple Statistical Tests</div> <div class="author"> Yaohua Wang, <em>Fangyi Zhang</em>, Ming Lin, Senzhang Wang, Xiuyu Sun, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Rong Jin' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/cf7700139af1fa346d2f57f1f5c26c18-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Graph structure learning aims to learn connectivity in a graph from data. It is particularly important for many computer vision related tasks since no explicit graph structure is available for images for most cases. A natural way to construct a graph among images is to treat each image as a node and assign pairwise image similarities as weights to corresponding edges. It is well known that pairwise similarities between images are sensitive to the noise in feature representations, leading to unreliable graph structures. We address this problem from the viewpoint of statistical tests. By viewing the feature vector of each node as an independent sample, the decision of whether creating an edge between two nodes based on their similarity in feature representation can be thought as a single statistical test. To improve the robustness in the decision of creating an edge, multiple samples are drawn and integrated by multiple statistical tests to generate a more reliable similarity measure, consequentially more reliable graph structure. The corresponding elegant matrix form named -Attention is designed for efficiency. The effectiveness of multiple tests for graph structure learning is verified both theoretically and empirically on multiple clustering and ReID benchmark datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Face_Clustering_via-480.webp 480w,/assets/img/publication_preview/Face_Clustering_via-800.webp 800w,/assets/img/publication_preview/Face_Clustering_via-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Face_Clustering_via.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Face_Clustering_via.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2022adanets" class="col-sm-8"> <div class="title">Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space</div> <div class="author"> Yaohua Wang, Yaobin Zhang, <em>Fangyi Zhang</em>, Senzhang Wang, Ming Lin, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Yuqi Zhang, Xiuyu Sun' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2202.03800" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Face clustering has attracted rising research interest recently to take advantage of massive amounts of face images on the web. State-of-the-art performance has been achieved by Graph Convolutional Networks (GCN) due to their powerful representation capacity. However, existing GCN-based methods build face graphs mainly according to kNN relations in the feature space, which may lead to a lot of noise edges connecting two faces of different classes. The face features will be polluted when messages pass along these noise edges, thus degrading the performance of GCNs. In this paper, a novel algorithm named Ada-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In Ada-NETS, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCNs to cluster faces. Experiments on multiple public clustering datasets show that Ada-NETS significantly outperforms current state-of-the-art methods, proving its superiority and generalization.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Jmpnet-480.webp 480w,/assets/img/publication_preview/Jmpnet-800.webp 800w,/assets/img/publication_preview/Jmpnet-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Jmpnet.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jmpnet.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="9746149" class="col-sm-8"> <div class="title">Jmpnet: Joint Motion Prediction for Learning-Based Video Compression</div> <div class="author"> Dongyang Li, Zhenhong Sun, Zhiyu Tan, Xiuyu Sun, <em>Fangyi Zhang</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Yichen Qian, Hao Li' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/9746149" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In recent years, more attention is attracted by learning-based approaches in the field of video compression. Recent methods of this kind normally consist of three major components: intra-frame network, motion prediction network, and residual network, among which the motion prediction part is particularly critical for video compression. Benefiting from the optical flow which enables dense motion prediction, recent methods have shown competitive performance compared with traditional codecs. However, problems such as tail shadow and background distortion in the predicted frame remain unsolved. To tackle these problems, JMPNet is introduced in this paper to provide more accurate motion information by using both optical flow and dynamic local filter as well as an attention map to further fuse these motion information in a smarter way. Experimental results show that the proposed method surpasses state-of-the-art (SOTA) rate-distortion (RD) performance in the most data-sets.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/GCN_Based_Linkage-480.webp 480w,/assets/img/publication_preview/GCN_Based_Linkage-800.webp 800w,/assets/img/publication_preview/GCN_Based_Linkage-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/GCN_Based_Linkage.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="GCN_Based_Linkage.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2021gcn" class="col-sm-8"> <div class="title">GCN-Based Linkage Prediction for Face Clusteringon Imbalanced Datasets: An Empirical Study</div> <div class="author"> Huafeng Yang, Xingjian Chen, <em>Fangyi Zhang</em>, Guangyue Hei, Yunjie Wang, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Rong Du' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Workshops of the International Joint Conference on Artificial Intelligence</em>, Apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.researchgate.net/publication/353043011_GCN-Based_Linkage_Prediction_for_Face_Clusteringon_Imbalanced_Datasets_An_Empirical_Study" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In recent years, benefiting from the expressivepower of Graph Convolutional Networks (GCNs),significant breakthroughs have been made in faceclustering. However, rare attention has been paidto GCN-based clustering on imbalanced data. Al-though imbalance problem has been extensivelystudied, the impact of imbalanced data on GCN-based linkage prediction task is quite different,which would cause problems in two aspects: im-balanced linkage labels and biased graph represen-tations. The problem of imbalanced linkage labelsis similar to that in image classification task, but thelatter is a particular problem in GCN-based clus-tering via linkage prediction. Significantly biasedgraph representations in training can cause catas-trophic overfitting of a GCN model. To tacklethese problems, we evaluate the feasibility of thoseexisting methods for imbalanced image classifica-tion problem on graphs with extensive experiments,and present a new method to alleviate the imbal-anced labels and also augment graph representa-tions using a Reverse-Imbalance Weighted Sam-pling (RIWS) strategy, followed with insightfulanalyses and discussions. A series of imbalancedbenchmark datasets synthesized from MS-Celeb-1M and DeepFashion will be openly available.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Interpolation_Variable-480.webp 480w,/assets/img/publication_preview/Interpolation_Variable-800.webp 800w,/assets/img/publication_preview/Interpolation_Variable-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Interpolation_Variable.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Interpolation_Variable.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3474085.3475698" class="col-sm-8"> <div class="title">Interpolation Variable Rate Image Compression</div> <div class="author"> Zhenhong Sun, Zhiyu Tan, Xiuyu Sun, <em>Fangyi Zhang</em>, Yichen Qian, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Dongyang Li, Hao Li' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In ACM International Conference on Multimedia</em>, Virtual Event, China, Apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2109.09280" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Compression standards have been used to reduce the cost of image storage and transmission for decades. In recent years, learned image compression methods have been proposed and achieved compelling performance to the traditional standards. However, in these methods, a set of different networks are used for various compression rates, resulting in a high cost in model storage and training. Although some variable-rate approaches have been proposed to reduce the cost by using a single network, most of them brought some performance degradation when applying fine rate control. To enable variable-rate control without sacrificing the performance, we propose an efficient Interpolation Variable-Rate (IVR) network, by introducing a handy Interpolation Channel Attention (InterpCA) module in the compression network. With the use of two hyperparameters for rate control and linear interpolation, the InterpCA achieves a fine PSNR interval of 0.001 dB and a fine rate interval of 0.0001 Bits-Per-Pixel (BPP) with 9000 rates in the IVR network. Experimental results demonstrate that the IVR network is the first variable-rate learned method that outperforms VTM 9.0 (intra) in PSNR and Multiscale Structural Similarity (MS-SSIM).</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Adversarial_discriminative-480.webp 480w,/assets/img/publication_preview/Adversarial_discriminative-800.webp 800w,/assets/img/publication_preview/Adversarial_discriminative-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Adversarial_discriminative.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Adversarial_discriminative.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2019adversarial" class="col-sm-8"> <div class="title">Adversarial discriminative sim-to-real transfer of visuo-motor policies</div> <div class="author"> <em>Fangyi Zhang</em>, Jürgen Leitner, Zongyuan Ge, Michael Milford, and Peter Corke </div> <div class="periodical"> <em>The International Journal of Robotics Research (IJRR)</em>, Apr 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1709.05746" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Various approaches have been proposed to learn visuo-motor policies for real-world robotic applications. One solution is first learning in simulation then transferring to the real world. In the transfer, most existing approaches need real-world images with labels. However, the labelling process is often expensive or even impractical in many robotic applications. In this paper, we propose an adversarial discriminative sim-to-real transfer approach to reduce the cost of labelling real data. The effectiveness of the approach is demonstrated with modular networks in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The adversarial transfer approach reduced the labelled real data requirement by 50%. Policies can be transferred to real environments with only 93 labelled and 186 unlabelled real images. The transferred visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 97.8% success rate and 1.8 cm control accuracy.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/The_ACRV_picking-480.webp 480w,/assets/img/publication_preview/The_ACRV_picking-800.webp 800w,/assets/img/publication_preview/The_ACRV_picking-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/The_ACRV_picking.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="The_ACRV_picking.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="leitner2017acrv" class="col-sm-8"> <div class="title">The ACRV picking benchmark: A robotic shelf picking benchmark to foster reproducible research</div> <div class="author"> Jürgen Leitner, Adam W. Tow, Niko Sünderhauf, Jake E. Dean, Joseph W. Durham, and <span class="more-authors" title="click to view 13 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '13 more authors' ? 'Matthew Cooper, Markus Eich, Christopher Lehnert, Ruben Mangels, Christopher McCool, Peter T. Kujala, Lachlan Nicholson, Trung Pham, James Sergeant, Liao Wu, Fangyi Zhang, Ben Upcroft, Peter Corke' : '13 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">13 more authors</span> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, Apr 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1609.05258" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA Challenges are an established and important way to drive scientific progress. They make research comparable on a well-defined benchmark with equal test conditions for all participants. However, such challenge events occur only occasionally, are limited to a small number of contestants, and the test conditions are very difficult to replicate after the main event. We present a new physical benchmark challenge for robotic picking: the ACRV Picking Benchmark (APB). Designed to be reproducible, it consists of a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. A well-defined evaluation protocol enables the comparison of \emphcomplete robotic systems – including perception and manipulation – instead of sub-systems only. Our paper also describes and reports results achieved by an open baseline system based on a Baxter robot.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Modular_Deep_Q-480.webp 480w,/assets/img/publication_preview/Modular_Deep_Q-800.webp 800w,/assets/img/publication_preview/Modular_Deep_Q-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Modular_Deep_Q.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Modular_Deep_Q.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2017acra" class="col-sm-8"> <div class="title">Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies</div> <div class="author"> <em>Fangyi Zhang</em>, Jürgen Leitner, Michael Milford, and Peter Corke </div> <div class="periodical"> <em>In Australasian Conference on Robotics and Automation (ACRA)</em>, Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1610.06781" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>While deep learning has had significant successes in computer vision thanks to the abundance of visual data, collecting sufficiently large real-world datasets for robot learning can be costly. To increase the practicality of these techniques on real robots, we propose a modular deep reinforcement learning method capable of transferring models trained in simulation to a real-world robotic task. We introduce a bottleneck between perception and control, enabling the networks to be trained independently, but then merged and fine-tuned in an end-to-end manner to further improve hand-eye coordination. On a canonical, planar visually-guided robot reaching task a fine-tuned accuracy of 1.6 pixels is achieved, a significant improvement over naive transfer (17.5 pixels), showing the potential for more complicated and broader applications. Our method provides a technique for more efficient learning and transfer of visuo-motor policies for real robotic systems without relying entirely on large real-world robot datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Tuning_Modular-480.webp 480w,/assets/img/publication_preview/Tuning_Modular-800.webp 800w,/assets/img/publication_preview/Tuning_Modular-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Tuning_Modular.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Tuning_Modular.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2017cvpr" class="col-sm-8"> <div class="title">Tuning Modular Networks With Weighted Losses for Hand-Eye Coordination</div> <div class="author"> <em>Fangyi Zhang</em>, Jürgen Leitner, Michael Milford, and Peter Corke </div> <div class="periodical"> <em>In IEEE Conference on Computer Vision and Pattern Recognition Workshops</em>, Jul 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1705.05116" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces an end-to-end fine-tuning method to improve hand-eye coordination in modular deep visuo-motor policies (modular networks) where each module is trained independently. Benefiting from weighted losses, the fine-tuning method significantly improves the performance of the policies for a robotic planar reaching task.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/let_light_guide-480.webp 480w,/assets/img/publication_preview/let_light_guide-800.webp 800w,/assets/img/publication_preview/let_light_guide-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/let_light_guide.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="let_light_guide.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qiu2016let" class="col-sm-8"> <div class="title">Let the light guide us: VLC-based localization</div> <div class="author"> Kejie Qiu, <em>Fangyi Zhang</em>, and Ming Liu </div> <div class="periodical"> <em>IEEE Robotics &amp; Automation Magazine</em>, Jul 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/7737037" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We propose to use visible-light beacons for low-cost indoor localization. Modulated light-emitting diode (LED) lights are adapted for localization as well as illumination. The proposed solution consists of two components: light-signal decomposition and Bayesian localization.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Towards_Vision_Based-480.webp 480w,/assets/img/publication_preview/Towards_Vision_Based-800.webp 800w,/assets/img/publication_preview/Towards_Vision_Based-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Towards_Vision_Based.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Towards_Vision_Based.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2015towards" class="col-sm-8"> <div class="title">Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control</div> <div class="author"> <em>Fangyi Zhang</em>, Jürgen Leitner, Michael Milford, Ben Upcroft, and Peter Corke </div> <div class="periodical"> <em>In Australasian Conference on Robotics and Automation (ACRA)</em>, Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1511.03791" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/signal_decomp-480.webp 480w,/assets/img/publication_preview/signal_decomp-800.webp 800w,/assets/img/publication_preview/signal_decomp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/signal_decomp.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="signal_decomp.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2015asynchronous" class="col-sm-8"> <div class="title">Asynchronous blind signal decomposition using tiny-length code for visible light communication-based indoor localization</div> <div class="author"> <em>Fangyi Zhang</em>, Kejie Qiu, and Ming Liu </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/7139580" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Indoor localization is a fundamental capability for service robots and indoor applications on mobile devices. To realize that, the cost and performance are of great concern. In this paper, we introduce a lightweight signal encoding and decomposition method for a low-cost and low-power Visible Light Communication (VLC)-based indoor localization system. Firstly, a Gold-sequence-based tiny-length code selection method is introduced for light encoding. Then a correlation-based asynchronous blind light-signal decomposition method is developed for the decomposition of the lights mixed with modulated light sources. It is able to decompose the mixed light-signal package in real-time. The average decomposition time-cost for each frame is 20 ms. By using the decomposition results, the localization system achieves accuracy at 0.56 m. These features outperform other existing low-cost indoor localization approaches, such as WiFiSLAM.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vlc_loc-480.webp 480w,/assets/img/publication_preview/vlc_loc-800.webp 800w,/assets/img/publication_preview/vlc_loc-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/vlc_loc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vlc_loc.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qiu2015visible" class="col-sm-8"> <div class="title">Visible light communication-based indoor localization using Gaussian process</div> <div class="author"> Kejie Qiu, <em>Fangyi Zhang</em>, and Ming Liu </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems</em>, Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/7353809" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>For mobile robots and position-based services, such as healthcare service, precise localization is the most fundamental capability while low-cost localization solutions are with increasing need and potentially have a wide market. A low-cost localization solution based on a novel Visible Light Communication (VLC) system for indoor environments is proposed in this paper. A number of modulated LED lights are used as beacons to aid indoor localization additional to illumination. A Gaussian Process(GP) is used to model the intensity distributions of the light sources. A Bayesian localization framework is constructed using the results of the GP, leading to precise localization. Path-planning is hereby feasible by only using the GP variance field, rather than using a metric map. Dijkstra’s algorithm-based path-planner is adopted to cope with the practical situations. We demonstrate our localization system by real-time experiments performed on a tablet PC in an indoor environment.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vlc_path_planning-480.webp 480w,/assets/img/publication_preview/vlc_path_planning-800.webp 800w,/assets/img/publication_preview/vlc_path_planning-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/vlc_path_planning.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vlc_path_planning.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="7294062" class="col-sm-8"> <div class="title">Visible light communication-based indoor environment modeling and metric-free path planning</div> <div class="author"> Kejie Qiu, <em>Fangyi Zhang</em>, and Ming Liu </div> <div class="periodical"> <em>In IEEE International Conference on Automation Science and Engineering</em>, Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/7294062" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>For mobile robots and position-based services, localization is the most fundamental capability while path-planning is an important application based on that. A novel localization and path-planning solution based on a low-cost Visible Light Communication (VLC) system for indoor environments is proposed in this paper. A number of modulated LED lights are used as beacons to aid indoor localization additional to illumination. A Gaussian Process (GP) is used to model the intensity distributions of the light sources. Path-planning is hereby feasible by using the GP variance field, rather than using a metric map. Graph-based path-planners are introduced to cope with the practical situations. We demonstrate our path-planning system by real-time experiments performed on a tablet PC in an indoor environment.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Fangyi Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/05/01/tabs.html"}},{id:"news-our-paper-re-evaluating-parallel-finger-tip-tactile-sensing-for-inferring-object-adjectives-an-empirical-study-was-presented-in-iros-2023",title:"Our paper \u201cRe-Evaluating Parallel Finger-Tip Tactile Sensing for Inferring Object Adjectives: An Empirical...",description:"",section:"News"},{id:"news-our-paper-towards-assessing-compliant-robotic-grasping-from-first-object-perspective-via-instrumented-objects-got-accepted-to-ra-l",title:"Our paper \u201cTowards assessing compliant robotic grasping from first-object perspective via instrumented objects\u201d...",description:"",section:"News"},{id:"news-our-4th-workshop-on-representing-and-manipulating-deformable-objects-will-be-held-at-icra-2024-on-the-17th-of-may-in-pacifico-yokohama-north-room-g304",title:"Our 4th Workshop on Representing and Manipulating Deformable Objects will be held at...",description:"",section:"News"},{id:"news-our-ra-l-paper-towards-assessing-compliant-robotic-grasping-from-first-object-perspective-via-instrumented-objects-will-be-presented-at-the-40th-anniversary-of-the-ieee-conference-on-robotics-and-automation-icra-40",title:"Our RA-L paper \u201cTowards assessing compliant robotic grasping from first-object perspective via instrumented...",description:"",section:"News"},{id:"projects-learning-real-world-visuo-motor-policies-from-simulation",title:"Learning Real-world Visuo-motor Policies from Simulation",description:"This is my Ph.D. project in the Australian Centre for Robotic Vision at QUT, with supervisions from Prof. Peter Corke, Dr. J\xfcrgen Leitner, Prof. Michael Milford and Dr. Ben Upcroft.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-robotic-manipulation-for-warehouse-and-household-applications",title:"Robotic Manipulation for Warehouse and Household Applications",description:"Amazon Picking Challenge (2016) As part of the Team ACRV for the Amazon Picking Challenge 2016, I worked on hand-eye calibration with Dr. Leo Wu. Household Applications This is a project I worked on during my visit to the Perception and Robotics Group at the University of Maryland, College Park, Sep-Dec 2016.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-vlc-based-indoor-localization",title:"VLC-Based Indoor Localization",description:"This is a project I worked on with Prof. Ming Liu and Mr. Kejie Qiu when I was a research assistant in the RAM-LAB at HKUST. The project developed a beacon code selection algorithm and a decomposition algorithm for blindly mixed beacon signals, based on CDMA code selection principles and Gold-sequence correlation properties.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-tactile-based-robotic-manipulation",title:"Tactile-based Robotic Manipulation",description:"A project exploring tactile sensing and tactile-based robotic manipulation.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-assessing-compliant-robotic-manipulation-with-instrumented-objects",title:"Assessing Compliant Robotic Manipulation with Instrumented Objects",description:"A project investigating using instrumented objects to assess compliant robotic manipulation performance.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%64%72.%66%61%6E%67%79%69.%7A%68%61%6E%67@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0003-3938-5377","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=5jFI06UAAAAJ","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Fangyi_Zhang/","_blank")}},{id:"socials-scopus",title:"Scopus",section:"Socials",handler:()=>{window.open("https://www.scopus.com/authid/detail.uri?authorId=56742468200","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/fangyizhang-x","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/fangyi-zhang-a6108088","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>